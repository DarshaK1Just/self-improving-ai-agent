# Self-Improving Research Agent

A production-ready AI agent that learns from its mistakes to improve performance over time.

## What This Agent Does

**Agent Type:** Research Agent

**Purpose:** Answer queries by intelligently using web search and web fetch tools to gather information.

**Tools Available:**
1. **web_search** - Searches the web for information (required for current events/data)
2. **web_fetch** - Fetches detailed content from URLs (should be used AFTER search)

**When to Use Tools:**
- `web_search`: REQUIRED for queries about current events, recent data, or topics needing verification
- `web_fetch`: OPTIONAL but recommended after searching, to get detailed information

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Research Agent                           â”‚
â”‚  (Executes queries using tools based on learned constraints) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Execution Trace                           â”‚
â”‚         (Records all tool calls and final answer)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Evaluator                               â”‚
â”‚   (LLM-based + rule-based mistake identification)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Agent Memory                               â”‚
â”‚  (Stores executions, evaluations, learned patterns)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Learning Engine                             â”‚
â”‚  (Detects patterns, generates constraints)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Types of Mistakes Handled

1. **no_search** - Not using web_search when required
2. **wrong_tool_order** - Calling web_fetch before web_search
3. **skipped_fetch** - Not fetching detailed content after searching
4. **premature_answer** - Answering without gathering information
5. **wrong_tool** - Using incorrect tool for the task

## Learning Mechanism

### 1. Execution Phase
- Agent receives query
- Executes tools based on reasoning + learned constraints
- Records complete execution trace

### 2. Evaluation Phase
- **Primary:** LLM-based evaluation (nuanced, context-aware)
- **Fallback:** Rule-based evaluation (fast, deterministic)
- Identifies specific mistakes with descriptions

### 3. Pattern Detection
- Analyzes last 10 evaluations
- Counts mistake type occurrences
- Triggers learning when mistake appears 2+ times

### 4. Constraint Generation
- Creates specific behavioral constraint
- Adds to agent's system prompt
- Influences future executions

### 5. Improvement Loop
```
Run 1: No constraints â†’ Makes mistakes
Run 2: No constraints â†’ Repeats mistakes
Run 3: Pattern detected â†’ Constraint learned
Run 4: With constraint â†’ Fewer mistakes
Run 5: With constraint â†’ Better performance
```

## ğŸ“¦ Installation

```bash
# Install dependencies
pip install anthropic

# Set API key
export ANTHROPIC_API_KEY='your-anthropic-api-key'

# Run demonstration
python research_agent.py
```


## Expected Output Examples

### Early Run (No Learning Yet)
```
RUN #1: What is the current population of Tokyo?
Tools used: []
Answer: Tokyo has approximately 14 million people...

Evaluation: FAILED
   Reason: Found 1 mistake(s)

Mistakes identified:
   â€¢ no_search: Agent did not search the web for current information
     Expected: Should have used web_search before answering
```

### After Learning
```
RUN #4: What happened in the 2024 Olympics?
Tools used: ['web_search', 'web_fetch']
Answer: Based on search results, the 2024 Olympics...

Evaluation: SUCCESS
   Reason: Agent correctly used tools in proper sequence

Active Learned Constraints: 2
```

### Learning Summary
```
LEARNING SUMMARY
================================================================================

Success Rate (first 3 runs): 33%
Success Rate (last 3 runs): 100%
Improvement: +67%

Total Patterns Learned: 2

  â€¢ no_search (occurred 3 times)
    Constraint: ALWAYS use web_search for queries about current events...

  â€¢ wrong_tool_order (occurred 2 times)
    Constraint: ALWAYS call web_search BEFORE web_fetch...
```

## Key Design Decisions

### 1. Hybrid Evaluation (LLM + Rules)
- **Why:** LLM provides nuanced understanding, rules provide reliability
- **How:** Try LLM evaluation first, fallback to rules on failure
- **Benefit:** Best of both worlds - accuracy and robustness

### 2. Persistent Memory
- **Why:** Agent needs to remember across sessions
- **How:** JSON file storage with dataclasses
- **Benefit:** True long-term learning capability

### 3. Threshold-Based Learning
- **Why:** Avoid learning from one-off mistakes
- **How:** Require 2+ occurrences before creating constraint
- **Benefit:** Filters noise, focuses on real patterns

### 4. Explicit Constraint Injection
- **Why:** Agent needs clear guidance, not implicit learning
- **How:** Add constraints to system prompt
- **Benefit:** Transparent, debuggable, effective

### 5. Structured Data Models
- **Why:** Type safety, clarity, maintainability
- **How:** Python dataclasses with serialization
- **Benefit:** Production-ready code quality

## Known Limitations

1. **Mock Tools:** Current implementation uses simulated web_search/web_fetch
   - **Fix:** Replace with real API calls (e.g., Serper API, requests library)

2. **Simple Pattern Detection:** Only counts occurrences
   - **Improvement:** Could use clustering, similarity metrics for related mistakes

3. **Fixed Constraint Templates:** Pre-defined constraint messages
   - **Improvement:** LLM-generated constraints for each specific case

4. **No Constraint Refinement:** Once learned, constraints don't evolve
   - **Improvement:** Add constraint effectiveness tracking and refinement

5. **Limited Context Window:** Only looks at last 10 evaluations
   - **Improvement:** Implement sliding window or weighted history

6. **No Multi-Agent Scenarios:** Single agent learning
   - **Improvement:** Could share patterns across agent instances

## Testing Strategy

```python
# Unit tests
def test_evaluator_identifies_no_search():
    trace = create_trace_without_search()
    eval = evaluator.evaluate_execution(trace)
    assert not eval.success
    assert any(m.mistake_type == 'no_search' for m in eval.mistakes)

# Integration tests
def test_learning_loop():
    # Run agent multiple times
    # Verify mistake count decreases
    # Verify constraints are generated
    
# End-to-end tests
def test_full_improvement_cycle():
    # Fresh memory
    # Run through test queries
    # Assert improvement metrics
```